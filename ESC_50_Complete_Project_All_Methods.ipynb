{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECEN 758 Data Mining and Analysis Project - Fall 2025\n",
    "\n",
    "**Due Date:** November 25, 2025, 11:59 PM CST\n",
    "\n",
    "**Team Members:**\n",
    "- Banghyon Lee (Joseph)\n",
    "- Fahimeh Orvati Nia\n",
    "- Nandhini Valiveti\n",
    "- Sushama Perati\n",
    "\n",
    "**Dataset:** ESC-50\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook contains the complete ESC-50 Environmental Sound Classification project, combining all team members' work:\n",
    "\n",
    "1. **Feature Extraction and Preprocessing** \n",
    "   - Raw features (176 features: MFCC, Chroma, Mel, Contrast, Tonnetz)\n",
    "   - Normalized features (StandardScaler)\n",
    "   - Selected features (Top 100 via Mutual Information)\n",
    "\n",
    "2. **Model Implementations:**\n",
    "   - **Fahimeh Orvati Nia:** Naive Bayes, CNN\n",
    "   - **Banghyon Lee (Joseph):** \n",
    "     - Random Forest, Gradient Boosting, XGBoost (on traditional features)\n",
    "     - Transfer Learning with YAMNet + Random Forest\n",
    "     - MLP on YAMNet embeddings\n",
    "     - Self-Supervised Learning (Pseudo-Labeling) with threshold sweep\n",
    "   - **Nandhini Valiveti:** SVM (Linear & RBF), MLP\n",
    "   - **Sushama Perati:** Logistic Regression, KNN\n",
    "\n",
    "3. **Evaluation:** \n",
    "   - All models tested on Raw, Normalized, and Selected feature sets\n",
    "   - Comprehensive metrics: Accuracy, Precision, Recall, F1-score\n",
    "   - Visualizations: Confusion matrices, learning curves, feature importance, performance heatmaps\n",
    "\n",
    "## ESC-50 Dataset\n",
    "\n",
    "The ESC-50 dataset is a labeled collection of 2000 environmental sound recordings, each 5 seconds long. It is divided into 50 classes, with 40 examples per class. The dataset is pre-arranged into 5 folds for cross-validation (folds 1-3: train, fold 4: validation, fold 5: test).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Data Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/scratch/user/fahimehorvatinia/cursor/cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q librosa scikit-learn xgboost tensorflow tensorflow-hub soundfile pandas numpy matplotlib seaborn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download the ESC-50 dataset\n",
    "!wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip -O ESC-50.zip\n",
    "!unzip -q ESC-50.zip\n",
    "!mv ESC-50-master ESC-50_data\n",
    "print(\"Dataset downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction and Preprocessing \n",
    "\n",
    "This section extracts features from audio files using librosa and creates three datasets:\n",
    "- **Raw features:** 176 features (MFCC, Chroma, Mel, Contrast, Tonnetz)\n",
    "- **Normalized features:** StandardScaler applied to raw features\n",
    "- **Selected features:** Top 100 features selected using Mutual Information\n",
    "\n",
    "**Note:** If feature files already exist, you can skip this section and load them directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature files already exist. Loading from disk...\n",
      "Loaded raw features: (2000, 177)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "META_CSV = 'ESC-50_data/meta/esc50.csv'\n",
    "AUDIO_DIR = 'ESC-50_data/audio'\n",
    "os.makedirs(\"data_out\", exist_ok=True)\n",
    "\n",
    "# Check if features already exist\n",
    "if os.path.exists(\"data_out/esc50_features_raw.csv\"):\n",
    "    print(\"Feature files already exist. Loading from disk...\")\n",
    "    df = pd.read_csv(\"data_out/esc50_features_raw.csv\")\n",
    "    print(f\"Loaded raw features: {df.shape}\")\n",
    "else:\n",
    "    print(\"Extracting features from audio files...\")\n",
    "    # Load metadata\n",
    "    meta = pd.read_csv(META_CSV)\n",
    "    print(\"Loaded:\", meta.shape, \"files\")\n",
    "\n",
    "    # Feature extraction function\n",
    "    def extract_features(file_path):\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if len(y) > sr*5:\n",
    "            y = y[:sr*5]\n",
    "\n",
    "        # Core features\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr), axis=1)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr), axis=1)\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr), axis=1)\n",
    "\n",
    "        features = np.hstack([mfcc, chroma, mel, contrast, tonnetz])\n",
    "        return features\n",
    "\n",
    "    # Extract features for all audio files\n",
    "    all_features = []\n",
    "    for _, row in tqdm(meta.iterrows(), total=len(meta), desc=\"Extracting features\"):\n",
    "        file_path = os.path.join(AUDIO_DIR, row[\"filename\"])\n",
    "        feats = extract_features(file_path)\n",
    "        all_features.append({\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"target\": row[\"target\"],\n",
    "            \"label\": row[\"category\"],\n",
    "            \"fold\": row[\"fold\"],\n",
    "            **{f\"f{i}\": feats[i] for i in range(len(feats))}\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(all_features)\n",
    "    print(\"Raw feature matrix shape:\", df.shape)\n",
    "    df.to_csv(\"data_out/esc50_features_raw.csv\", index=False)\n",
    "    print(\"Saved raw feature file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized features already exist.\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "if not os.path.exists(\"data_out/esc50_features_normalized_corrected.csv\"):\n",
    "    print(\"Creating normalized features...\")\n",
    "    df_raw = pd.read_csv(\"data_out/esc50_features_raw.csv\")\n",
    "    df_features_only = df_raw.drop(columns=['filename', 'fold'])\n",
    "    feature_cols = [c for c in df_features_only.columns if c.startswith(\"f\")]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(df_features_only[feature_cols])\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_features, columns=feature_cols)\n",
    "    normalized_df['target'] = df_raw['target']\n",
    "    normalized_df['label'] = df_raw['label']\n",
    "    normalized_df['filename'] = df_raw['filename']\n",
    "    normalized_df['fold'] = df_raw['fold']\n",
    "\n",
    "    normalized_df.to_csv(\"data_out/esc50_features_normalized_corrected.csv\", index=False)\n",
    "    joblib.dump(scaler, \"data_out/scaler_corrected.pkl\")\n",
    "    print(\"Saved normalized dataset + scaler\")\n",
    "else:\n",
    "    print(\"Normalized features already exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features already exist.\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection using Mutual Information\n",
    "import re\n",
    "\n",
    "if not os.path.exists(\"data_out/esc50_features_selected.csv\"):\n",
    "    print(\"Creating selected features...\")\n",
    "    df_raw = pd.read_csv(\"data_out/esc50_features_raw.csv\")\n",
    "    # Use regex to match only numeric feature columns (f0, f1, f2, etc.), not \"filename\"\n",
    "    feature_cols = [c for c in df_raw.columns if re.fullmatch(r'f\\d+', c)]\n",
    "    feature_cols = sorted(feature_cols, key=lambda x: int(x[1:]))  # Sort by number\n",
    "    \n",
    "    X = df_raw[feature_cols].values.astype(np.float32)  # Convert to numpy array and ensure float\n",
    "    y = df_raw['target'].values.astype(int)\n",
    "    \n",
    "    print(f\"Computing mutual information for {len(feature_cols)} features...\")\n",
    "    mi = mutual_info_classif(X, y, random_state=42)\n",
    "    mi_series = pd.Series(mi, index=feature_cols).sort_values(ascending=False)\n",
    "    top_features = mi_series.head(100).index.tolist()\n",
    "\n",
    "    df_selected = df_raw[['filename', 'target', 'label', 'fold'] + top_features]\n",
    "    df_selected.to_csv(\"data_out/esc50_features_selected.csv\", index=False)\n",
    "    print(f\"Saved selected feature file: {df_selected.shape}\")\n",
    "    print(f\"Top 10 features by MI: {top_features[:10]}\")\n",
    "else:\n",
    "    print(\"Selected features already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Utility Functions\n",
    "\n",
    "Functions to load datasets and evaluate models consistently across all team members' implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RAW from data_out/esc50_features_raw.csv -> shape = (2000, 177)\n",
      "RAW: X.shape = (2000, 173), y.shape = (2000,), num_features = 173\n",
      "Loaded NORMALIZED from data_out/esc50_features_normalized_corrected.csv -> shape = (2000, 177)\n",
      "NORMALIZED: X.shape = (2000, 173), y.shape = (2000,), num_features = 173\n",
      "Loaded SELECTED from data_out/esc50_features_selected.csv -> shape = (2000, 104)\n",
      "SELECTED: X.shape = (2000, 100), y.shape = (2000,), num_features = 100\n",
      "\n",
      "Total classes: 50\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Utility Functions\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_esc50_features(path, name=\"\"):\n",
    "    \"\"\"Load feature CSV and return X, y, labels, feature_cols, folds\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {name} from {path} -> shape = {df.shape}\")\n",
    "\n",
    "    non_feature_cols = [c for c in [\"filename\", \"target\", \"label\", \"fold\"] if c in df.columns]\n",
    "    feature_cols = [c for c in df.columns if re.fullmatch(r'f\\d+', c)]\n",
    "    feature_cols = sorted(feature_cols, key=lambda x: int(x[1:]))\n",
    "\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y = df[\"target\"].values.astype(int)\n",
    "    labels = df[\"label\"].values if \"label\" in df.columns else None\n",
    "    folds = df[\"fold\"].values if \"fold\" in df.columns else None\n",
    "\n",
    "    print(f\"{name}: X.shape = {X.shape}, y.shape = {y.shape}, num_features = {len(feature_cols)}\")\n",
    "    return df, X, y, labels, feature_cols, folds\n",
    "\n",
    "def split_by_folds(X, y, folds, verbose=True):\n",
    "    \"\"\"\n",
    "    Split data using ESC-50 folds: 1-3 train, 4 val, 5 test\n",
    "    Ensures all classes are present in train, validation, and test sets.\n",
    "    CRITICAL: Ensures NO data leakage - train samples never appear in val/test.\n",
    "    If ESC-50 folds don't guarantee all classes, uses stratified splitting on ENTIRE dataset.\n",
    "    \"\"\"\n",
    "    folds = folds.astype(int)\n",
    "    train_mask = np.isin(folds, [1, 2, 3])\n",
    "    val_mask = (folds == 4)\n",
    "    test_mask = (folds == 5)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    # Check if all classes are present in all splits\n",
    "    unique_classes = np.unique(y)\n",
    "    train_classes = np.unique(y_train)\n",
    "    val_classes = np.unique(y_val)\n",
    "    test_classes = np.unique(y_test)\n",
    "    \n",
    "    all_classes_in_train = len(train_classes) == len(unique_classes)\n",
    "    all_classes_in_val = len(val_classes) == len(unique_classes)\n",
    "    all_classes_in_test = len(test_classes) == len(unique_classes)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Data Split Verification\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total classes: {len(unique_classes)}\")\n",
    "        print(f\"Train classes: {len(train_classes)}/{len(unique_classes)} - {'✓ All present' if all_classes_in_train else '✗ Missing classes'}\")\n",
    "        print(f\"Val classes:   {len(val_classes)}/{len(unique_classes)} - {'✓ All present' if all_classes_in_val else '✗ Missing classes'}\")\n",
    "        print(f\"Test classes:  {len(test_classes)}/{len(unique_classes)} - {'✓ All present' if all_classes_in_test else '✗ Missing classes'}\")\n",
    "    \n",
    "    # If any split is missing classes, use stratified splitting on ENTIRE dataset\n",
    "    # This ensures no data leakage - we split the entire dataset fresh\n",
    "    if not (all_classes_in_train and all_classes_in_val and all_classes_in_test):\n",
    "        if verbose:\n",
    "            print(\"\\n⚠ ESC-50 folds don't guarantee all classes in all splits.\")\n",
    "            print(\"Using stratified splitting on ENTIRE dataset to ensure class balance...\")\n",
    "            print(\"⚠️  WARNING: This will create NEW splits, not using ESC-50 fold structure\")\n",
    "        \n",
    "        # Split ENTIRE dataset (X, y) into train+val (80%) and test (20%) - stratified\n",
    "        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "        X_trainval, y_trainval = X[trainval_idx], y[trainval_idx]\n",
    "        X_test_new, y_test_new = X[test_idx], y[test_idx]\n",
    "        \n",
    "        # Split train+val into train (75%) and val (25%) - stratified\n",
    "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "        train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "        X_train_new, y_train_new = X_trainval[train_idx], y_trainval[train_idx]\n",
    "        X_val_new, y_val_new = X_trainval[val_idx], y_trainval[val_idx]\n",
    "        \n",
    "        # Verify no overlap (data leakage check)\n",
    "        # Verify no overlap using array views (more robust check)\n",
    "        # Check if arrays share memory or have identical rows\n",
    "        # Since we use indices from StratifiedShuffleSplit, there should be no overlap\n",
    "        # But we verify by checking if any train sample appears in val/test\n",
    "        train_indices_set = set(train_idx)\n",
    "        val_indices_set = set(val_idx)\n",
    "        test_indices_set = set(test_idx)\n",
    "        \n",
    "        overlap_train_val = len(train_indices_set.intersection(val_indices_set))\n",
    "        overlap_train_test = len(train_indices_set.intersection(test_indices_set))\n",
    "        overlap_val_test = len(val_indices_set.intersection(test_indices_set))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nAfter stratified split:\") \n",
    "            train_classes_new = np.unique(y_train_new)\n",
    "            val_classes_new = np.unique(y_val_new)\n",
    "            test_classes_new = np.unique(y_test_new)\n",
    "            print(f\"Train classes: {len(train_classes_new)}/{len(unique_classes)} - {'✓ All present' if len(train_classes_new) == len(unique_classes) else '✗ Missing classes'}\")\n",
    "            print(f\"Val classes:   {len(val_classes_new)}/{len(unique_classes)} - {'✓ All present' if len(val_classes_new) == len(unique_classes) else '✗ Missing classes'}\")\n",
    "            print(f\"Test classes:  {len(test_classes_new)}/{len(unique_classes)} - {'✓ All present' if len(test_classes_new) == len(unique_classes) else '✗ Missing classes'}\")\n",
    "            print(f\"\\nSplit sizes: Train={X_train_new.shape[0]}, Val={X_val_new.shape[0]}, Test={X_test_new.shape[0]}\")\n",
    "            print(f\"Data leakage check (sample overlap): Train-Val={overlap_train_val}, Train-Test={overlap_train_test}, Val-Test={overlap_val_test}\")\n",
    "            if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "                print(\"✓ No data leakage detected\")\n",
    "            else:\n",
    "                print(\"⚠️  WARNING: Potential data leakage detected!\")\n",
    "        \n",
    "        return X_train_new, X_val_new, X_test_new, y_train_new, y_val_new, y_test_new\n",
    "    \n",
    "    # Verify no overlap when using ESC-50 folds (sanity check)\n",
    "    if verbose:\n",
    "        # Verify no overlap - ESC-50 folds are mutually exclusive by design\n",
    "        # Folds 1-3 (train), 4 (val), 5 (test) should have no overlap\n",
    "        train_fold_set = set(np.where(train_mask)[0])\n",
    "        val_fold_set = set(np.where(val_mask)[0])\n",
    "        test_fold_set = set(np.where(test_mask)[0])\n",
    "        \n",
    "        overlap_train_val = len(train_fold_set.intersection(val_fold_set))\n",
    "        overlap_train_test = len(train_fold_set.intersection(test_fold_set))\n",
    "        overlap_val_test = len(val_fold_set.intersection(test_fold_set))\n",
    "        \n",
    "        print(f\"\\nData leakage check (sample overlap): Train-Val={overlap_train_val}, Train-Test={overlap_train_test}, Val-Test={overlap_val_test}\")\n",
    "        if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "            print(\"✓ No data leakage - ESC-50 folds maintain strict separation\")\n",
    "        else:\n",
    "            print(\"⚠️  WARNING: Potential data leakage detected in ESC-50 folds!\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✓ All classes present in all splits using ESC-50 folds\")\n",
    "        print(f\"Split sizes: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_name, dataset_name):\n",
    "    \"\"\"Train and evaluate model, return metrics\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    val_prec = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "    val_rec = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "\n",
    "    test_prec = precision_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "    test_rec = recall_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Train_Accuracy': train_acc,\n",
    "        'Val_Accuracy': val_acc,\n",
    "        'Test_Accuracy': test_acc,\n",
    "        'Val_Precision': val_prec,\n",
    "        'Val_Recall': val_rec,\n",
    "        'Val_F1': val_f1,\n",
    "        'Test_Precision': test_prec,\n",
    "        'Test_Recall': test_rec,\n",
    "        'Test_F1': test_f1\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} on {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Val Precision: {val_prec:.4f}, Recall: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "    print(f\"Test Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return results, y_val_pred, y_test_pred\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title, figsize=(12, 10)):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=figsize, dpi=150)\n",
    "    sns.heatmap(cm, annot=False, cmap='Blues', square=True, cbar=True,\n",
    "                xticklabels=np.arange(len(classes)),\n",
    "                yticklabels=np.arange(len(classes)))\n",
    "    plt.xlabel('Predicted', fontsize=11)\n",
    "    plt.ylabel('True', fontsize=11)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load all three datasets\n",
    "raw_df, X_raw, y_raw, labels_raw, feat_raw, folds_raw = load_esc50_features(\n",
    "    \"data_out/esc50_features_raw.csv\", name=\"RAW\"\n",
    ")\n",
    "\n",
    "norm_df, X_norm, y_norm, labels_norm, feat_norm, folds_norm = load_esc50_features(\n",
    "    \"data_out/esc50_features_normalized_corrected.csv\", name=\"NORMALIZED\"\n",
    ")\n",
    "\n",
    "sel_df, X_sel, y_sel, labels_sel, feat_sel, folds_sel = load_esc50_features(\n",
    "    \"data_out/esc50_features_selected.csv\", name=\"SELECTED\"\n",
    ")\n",
    "\n",
    "# Get label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(labels_raw)\n",
    "class_names = le.classes_\n",
    "print(f\"\\nTotal classes: {len(class_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementations\n",
    "\n",
    "### 4.1 Sushama Perati: Logistic Regression and KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Logistic Regression on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.9533\n",
      "Val Accuracy: 0.3225\n",
      "Test Accuracy: 0.2875\n",
      "Val Precision: 0.3127, Recall: 0.3225, F1: 0.3056\n",
      "Test Precision: 0.2820, Recall: 0.2875, F1: 0.2743\n",
      "\n",
      "============================================================\n",
      "KNN on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.5358\n",
      "Val Accuracy: 0.2125\n",
      "Test Accuracy: 0.1625\n",
      "Val Precision: 0.2472, Recall: 0.2125, F1: 0.1881\n",
      "Test Precision: 0.2010, Recall: 0.1625, F1: 0.1531\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Logistic Regression on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.9233\n",
      "Val Accuracy: 0.4150\n",
      "Test Accuracy: 0.3275\n",
      "Val Precision: 0.4498, Recall: 0.4150, F1: 0.4046\n",
      "Test Precision: 0.3421, Recall: 0.3275, F1: 0.3152\n",
      "\n",
      "============================================================\n",
      "KNN on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.5933\n",
      "Val Accuracy: 0.2600\n",
      "Test Accuracy: 0.2250\n",
      "Val Precision: 0.2751, Recall: 0.2600, F1: 0.2350\n",
      "Test Precision: 0.2664, Recall: 0.2250, F1: 0.2037\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Logistic Regression on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.7817\n",
      "Val Accuracy: 0.3525\n",
      "Test Accuracy: 0.2900\n",
      "Val Precision: 0.3618, Recall: 0.3525, F1: 0.3414\n",
      "Test Precision: 0.2916, Recall: 0.2900, F1: 0.2740\n",
      "\n",
      "============================================================\n",
      "KNN on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.4850\n",
      "Val Accuracy: 0.1925\n",
      "Test Accuracy: 0.1650\n",
      "Val Precision: 0.1931, Recall: 0.1925, F1: 0.1681\n",
      "Test Precision: 0.1926, Recall: 0.1650, F1: 0.1552\n",
      "\n",
      "============================================================\n",
      "SUSHAMA PERATI - Summary Results\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Val_Precision</th>\n",
       "      <th>Val_Recall</th>\n",
       "      <th>Val_F1</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "      <th>Test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.312689</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.305626</td>\n",
       "      <td>0.281982</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.274294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.535833</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.247192</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.188081</td>\n",
       "      <td>0.200951</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.153120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>0.449847</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.404608</td>\n",
       "      <td>0.342148</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>0.315237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.275084</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.266414</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.203721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.781667</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.361831</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.341383</td>\n",
       "      <td>0.291598</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.274036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.193092</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.168113</td>\n",
       "      <td>0.192602</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.155205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model     Dataset  Train_Accuracy  Val_Accuracy  \\\n",
       "0  Logistic Regression         Raw        0.953333        0.3225   \n",
       "1                  KNN         Raw        0.535833        0.2125   \n",
       "2  Logistic Regression  Normalized        0.923333        0.4150   \n",
       "3                  KNN  Normalized        0.593333        0.2600   \n",
       "4  Logistic Regression    Selected        0.781667        0.3525   \n",
       "5                  KNN    Selected        0.485000        0.1925   \n",
       "\n",
       "   Test_Accuracy  Val_Precision  Val_Recall    Val_F1  Test_Precision  \\\n",
       "0         0.2875       0.312689      0.3225  0.305626        0.281982   \n",
       "1         0.1625       0.247192      0.2125  0.188081        0.200951   \n",
       "2         0.3275       0.449847      0.4150  0.404608        0.342148   \n",
       "3         0.2250       0.275084      0.2600  0.234973        0.266414   \n",
       "4         0.2900       0.361831      0.3525  0.341383        0.291598   \n",
       "5         0.1650       0.193092      0.1925  0.168113        0.192602   \n",
       "\n",
       "   Test_Recall   Test_F1  \n",
       "0       0.2875  0.274294  \n",
       "1       0.1625  0.153120  \n",
       "2       0.3275  0.315237  \n",
       "3       0.2250  0.203721  \n",
       "4       0.2900  0.274036  \n",
       "5       0.1650  0.155205  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Prepare datasets for Sushama's models\n",
    "datasets = {\n",
    "    'Raw': (X_raw, y_raw, folds_raw),\n",
    "    'Normalized': (X_norm, y_norm, folds_norm),\n",
    "    'Selected': (X_sel, y_sel, folds_sel)\n",
    "}\n",
    "\n",
    "sushama_results = []\n",
    "\n",
    "for dataset_name, (X, y, folds) in datasets.items():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_by_folds(X, y, folds)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        lr, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'Logistic Regression', dataset_name\n",
    "    )\n",
    "    sushama_results.append(res)\n",
    "    \n",
    "    # KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        knn, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'KNN', dataset_name\n",
    "    )\n",
    "    sushama_results.append(res)\n",
    "\n",
    "sushama_df = pd.DataFrame(sushama_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUSHAMA PERATI - Summary Results\")\n",
    "print(\"=\"*60)\n",
    "display(sushama_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fahimeh Orvati Nia: Naive Bayes and CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU configured successfully!\n",
      "  Found 1 GPU(s):\n",
      "    GPU 0: /physical_device:GPU:0\n",
      "  Memory growth: ENABLED\n",
      "  GPU test: SUCCESS - GPU is ready to use\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Naive Bayes on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.4150\n",
      "Val Accuracy: 0.2400\n",
      "Test Accuracy: 0.2250\n",
      "Val Precision: 0.2853, Recall: 0.2400, F1: 0.2310\n",
      "Test Precision: 0.2491, Recall: 0.2250, F1: 0.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CNN on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.9925\n",
      "Val Accuracy: 0.4300\n",
      "Test Accuracy: 0.3625\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Naive Bayes on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.4258\n",
      "Val Accuracy: 0.2250\n",
      "Test Accuracy: 0.2175\n",
      "Val Precision: 0.2576, Recall: 0.2250, F1: 0.2116\n",
      "Test Precision: 0.2490, Recall: 0.2175, F1: 0.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CNN on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.9975\n",
      "Val Accuracy: 0.4250\n",
      "Test Accuracy: 0.3675\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Naive Bayes on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.3992\n",
      "Val Accuracy: 0.2025\n",
      "Test Accuracy: 0.2000\n",
      "Val Precision: 0.2397, Recall: 0.2025, F1: 0.2045\n",
      "Test Precision: 0.2267, Recall: 0.2000, F1: 0.1907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CNN on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.8917\n",
      "Val Accuracy: 0.4150\n",
      "Test Accuracy: 0.3475\n",
      "\n",
      "============================================================\n",
      "FAHIMEH ORVATI NIA - Summary Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Val_Precision</th>\n",
       "      <th>Val_Recall</th>\n",
       "      <th>Val_F1</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "      <th>Test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.285273</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.231012</td>\n",
       "      <td>0.249106</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.205980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.408896</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.398817</td>\n",
       "      <td>0.361711</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.346535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.257610</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.211610</td>\n",
       "      <td>0.248963</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.200603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.428263</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.400330</td>\n",
       "      <td>0.366905</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.345646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.399167</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.239656</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.226697</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.190675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.436591</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.400813</td>\n",
       "      <td>0.355240</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.330901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model     Dataset  Train_Accuracy  Val_Accuracy  Test_Accuracy  \\\n",
       "0  Naive Bayes         Raw        0.415000        0.2400         0.2250   \n",
       "1          CNN         Raw        0.992500        0.4300         0.3625   \n",
       "2  Naive Bayes  Normalized        0.425833        0.2250         0.2175   \n",
       "3          CNN  Normalized        0.997500        0.4250         0.3675   \n",
       "4  Naive Bayes    Selected        0.399167        0.2025         0.2000   \n",
       "5          CNN    Selected        0.891667        0.4150         0.3475   \n",
       "\n",
       "   Val_Precision  Val_Recall    Val_F1  Test_Precision  Test_Recall   Test_F1  \n",
       "0       0.285273      0.2400  0.231012        0.249106       0.2250  0.205980  \n",
       "1       0.408896      0.4300  0.398817        0.361711       0.3625  0.346535  \n",
       "2       0.257610      0.2250  0.211610        0.248963       0.2175  0.200603  \n",
       "3       0.428263      0.4250  0.400330        0.366905       0.3675  0.345646  \n",
       "4       0.239656      0.2025  0.204470        0.226697       0.2000  0.190675  \n",
       "5       0.436591      0.4150  0.400813        0.355240       0.3475  0.330901  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ============================================\n",
    "# CRITICAL: GPU Configuration for CUDA\n",
    "# ============================================\n",
    "# Clear any existing TensorFlow state FIRST\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Configure GPU with memory growth BEFORE any operations\n",
    "# This prevents CUDA_ERROR_INVALID_HANDLE\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # IMPORTANT: Set memory growth BEFORE TensorFlow uses GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"✓ GPU configured successfully!\")\n",
    "        print(f\"  Found {len(gpus)} GPU(s):\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"    GPU {i}: {gpu.name}\")\n",
    "        print(f\"  Memory growth: ENABLED\")\n",
    "        \n",
    "        # Verify GPU is accessible\n",
    "        try:\n",
    "            with tf.device(\"/GPU:0\"):\n",
    "                test_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "                _ = test_tensor * 2\n",
    "            print(f\"  GPU test: SUCCESS - GPU is ready to use\")\n",
    "        except Exception as test_e:\n",
    "            print(f\"  GPU test: FAILED - {test_e}\")\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(f\"✗ GPU configuration failed: {e}\")\n",
    "        print(\"  Falling back to CPU...\")\n",
    "        import os\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "        tf.keras.backend.clear_session()\n",
    "else:\n",
    "    print(\"⚠ No GPU devices found\")\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Final session clear\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "fahimeh_results = []\n",
    "\n",
    "for dataset_name, (X, y, folds) in datasets.items():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_by_folds(X, y, folds)\n",
    "    \n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        nb, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'Naive Bayes', dataset_name\n",
    "    )\n",
    "    fahimeh_results.append(res)\n",
    "    \n",
    "    # CNN (1D CNN approach)\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Reshape for 1D CNN\n",
    "    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val_cnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    # Force CPU for CNN model to avoid CUDA kernel errors\n",
    "    # GPU kernel loading fails due to CUDA version mismatch (TF 12.5.1 vs System 12.8)\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Build CNN model on CPU\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        # Build CNN model\n",
    "        cnn_model = keras.Sequential([\n",
    "            layers.Conv1D(64, 3, activation='relu', input_shape=(input_dim, 1)),\n",
    "            layers.MaxPooling1D(2),\n",
    "            layers.Conv1D(128, 3, activation='relu'),\n",
    "            layers.MaxPooling1D(2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        cnn_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train CNN\n",
    "        history = cnn_model.fit(\n",
    "            X_train_cnn, y_train,\n",
    "            validation_data=(X_val_cnn, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate CNN\n",
    "        y_train_pred = np.argmax(cnn_model.predict(X_train_cnn, verbose=0), axis=1)\n",
    "        y_val_pred = np.argmax(cnn_model.predict(X_val_cnn, verbose=0), axis=1)\n",
    "        y_test_pred = np.argmax(cnn_model.predict(X_test_cnn, verbose=0), axis=1)\n",
    "    \n",
    "    # End of CPU device context - now compute metrics outside the with block\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    val_prec = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "    val_rec = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    \n",
    "    test_prec = precision_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "    test_rec = recall_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    \n",
    "    res = {\n",
    "        'Model': 'CNN',\n",
    "        'Dataset': dataset_name,\n",
    "        'Train_Accuracy': train_acc,\n",
    "        'Val_Accuracy': val_acc,\n",
    "        'Test_Accuracy': test_acc,\n",
    "        'Val_Precision': val_prec,\n",
    "        'Val_Recall': val_rec,\n",
    "        'Val_F1': val_f1,\n",
    "        'Test_Precision': test_prec,\n",
    "        'Test_Recall': test_rec,\n",
    "        'Test_F1': test_f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CNN on {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    fahimeh_results.append(res)\n",
    "\n",
    "fahimeh_df = pd.DataFrame(fahimeh_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAHIMEH ORVATI NIA - Summary Results\")\n",
    "print(\"=\"*60)\n",
    "display(fahimeh_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Nandhini Valiveti: SVM (Linear & RBF) and MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "SVM Linear on Raw\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.3675\n",
      "Test Accuracy: 0.2800\n",
      "Val Precision: 0.3855, Recall: 0.3675, F1: 0.3517\n",
      "Test Precision: 0.2738, Recall: 0.2800, F1: 0.2621\n",
      "\n",
      "============================================================\n",
      "SVM RBF on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.5525\n",
      "Val Accuracy: 0.3050\n",
      "Test Accuracy: 0.2675\n",
      "Val Precision: 0.3377, Recall: 0.3050, F1: 0.2860\n",
      "Test Precision: 0.2913, Recall: 0.2675, F1: 0.2624\n",
      "\n",
      "============================================================\n",
      "MLP on Raw\n",
      "============================================================\n",
      "Train Accuracy: 0.9192\n",
      "Val Accuracy: 0.3325\n",
      "Test Accuracy: 0.2625\n",
      "Val Precision: 0.3699, Recall: 0.3325, F1: 0.3212\n",
      "Test Precision: 0.2538, Recall: 0.2625, F1: 0.2447\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "SVM Linear on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.9925\n",
      "Val Accuracy: 0.4025\n",
      "Test Accuracy: 0.3125\n",
      "Val Precision: 0.4156, Recall: 0.4025, F1: 0.3886\n",
      "Test Precision: 0.3075, Recall: 0.3125, F1: 0.2965\n",
      "\n",
      "============================================================\n",
      "SVM RBF on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 0.9225\n",
      "Val Accuracy: 0.3950\n",
      "Test Accuracy: 0.3225\n",
      "Val Precision: 0.4361, Recall: 0.3950, F1: 0.3879\n",
      "Test Precision: 0.3236, Recall: 0.3225, F1: 0.3066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MLP on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.4300\n",
      "Test Accuracy: 0.3350\n",
      "Val Precision: 0.4401, Recall: 0.4300, F1: 0.4178\n",
      "Test Precision: 0.3367, Recall: 0.3350, F1: 0.3215\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "SVM Linear on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.9758\n",
      "Val Accuracy: 0.3625\n",
      "Test Accuracy: 0.3175\n",
      "Val Precision: 0.3547, Recall: 0.3625, F1: 0.3416\n",
      "Test Precision: 0.3176, Recall: 0.3175, F1: 0.3036\n",
      "\n",
      "============================================================\n",
      "SVM RBF on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.4725\n",
      "Val Accuracy: 0.2950\n",
      "Test Accuracy: 0.2350\n",
      "Val Precision: 0.3518, Recall: 0.2950, F1: 0.2738\n",
      "Test Precision: 0.2571, Recall: 0.2350, F1: 0.2225\n",
      "\n",
      "============================================================\n",
      "MLP on Selected\n",
      "============================================================\n",
      "Train Accuracy: 0.7375\n",
      "Val Accuracy: 0.3100\n",
      "Test Accuracy: 0.2700\n",
      "Val Precision: 0.3411, Recall: 0.3100, F1: 0.2903\n",
      "Test Precision: 0.3121, Recall: 0.2700, F1: 0.2539\n",
      "\n",
      "============================================================\n",
      "NANDHINI VALIVETI - Summary Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Val_Precision</th>\n",
       "      <th>Val_Recall</th>\n",
       "      <th>Val_F1</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "      <th>Test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Linear</td>\n",
       "      <td>Raw</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.385462</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.351685</td>\n",
       "      <td>0.273814</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.262053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM RBF</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.337674</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.285977</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.262449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Raw</td>\n",
       "      <td>0.919167</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.369891</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>0.321242</td>\n",
       "      <td>0.253756</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.244659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM Linear</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.4025</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.415622</td>\n",
       "      <td>0.4025</td>\n",
       "      <td>0.388624</td>\n",
       "      <td>0.307490</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.296453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM RBF</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.3950</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.436062</td>\n",
       "      <td>0.3950</td>\n",
       "      <td>0.387877</td>\n",
       "      <td>0.323612</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.306604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.440097</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.417773</td>\n",
       "      <td>0.336740</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.321475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM Linear</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.975833</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.354705</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.341569</td>\n",
       "      <td>0.317568</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.303617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM RBF</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.351757</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.273790</td>\n",
       "      <td>0.257087</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.222452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Selected</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.341114</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.290255</td>\n",
       "      <td>0.312093</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.253919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model     Dataset  Train_Accuracy  Val_Accuracy  Test_Accuracy  \\\n",
       "0  SVM Linear         Raw        1.000000        0.3675         0.2800   \n",
       "1     SVM RBF         Raw        0.552500        0.3050         0.2675   \n",
       "2         MLP         Raw        0.919167        0.3325         0.2625   \n",
       "3  SVM Linear  Normalized        0.992500        0.4025         0.3125   \n",
       "4     SVM RBF  Normalized        0.922500        0.3950         0.3225   \n",
       "5         MLP  Normalized        1.000000        0.4300         0.3350   \n",
       "6  SVM Linear    Selected        0.975833        0.3625         0.3175   \n",
       "7     SVM RBF    Selected        0.472500        0.2950         0.2350   \n",
       "8         MLP    Selected        0.737500        0.3100         0.2700   \n",
       "\n",
       "   Val_Precision  Val_Recall    Val_F1  Test_Precision  Test_Recall   Test_F1  \n",
       "0       0.385462      0.3675  0.351685        0.273814       0.2800  0.262053  \n",
       "1       0.337674      0.3050  0.285977        0.291258       0.2675  0.262449  \n",
       "2       0.369891      0.3325  0.321242        0.253756       0.2625  0.244659  \n",
       "3       0.415622      0.4025  0.388624        0.307490       0.3125  0.296453  \n",
       "4       0.436062      0.3950  0.387877        0.323612       0.3225  0.306604  \n",
       "5       0.440097      0.4300  0.417773        0.336740       0.3350  0.321475  \n",
       "6       0.354705      0.3625  0.341569        0.317568       0.3175  0.303617  \n",
       "7       0.351757      0.2950  0.273790        0.257087       0.2350  0.222452  \n",
       "8       0.341114      0.3100  0.290255        0.312093       0.2700  0.253919  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nandhini_results = []\n",
    "\n",
    "for dataset_name, (X, y, folds) in datasets.items():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_by_folds(X, y, folds)\n",
    "    \n",
    "    # SVM Linear\n",
    "    svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        svm_linear, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'SVM Linear', dataset_name\n",
    "    )\n",
    "    nandhini_results.append(res)\n",
    "    \n",
    "    # SVM RBF\n",
    "    svm_rbf = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        svm_rbf, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'SVM RBF', dataset_name\n",
    "    )\n",
    "    nandhini_results.append(res)\n",
    "    \n",
    "    # MLP\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(256,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        batch_size=64,\n",
    "        max_iter=200,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        mlp, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'MLP', dataset_name\n",
    "    )\n",
    "    nandhini_results.append(res)\n",
    "\n",
    "nandhini_df = pd.DataFrame(nandhini_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NANDHINI VALIVETI - Summary Results\")\n",
    "print(\"=\"*60)\n",
    "display(nandhini_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Banghyon Lee (Joseph): Random Forest, Gradient Boosting, XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Random Forest on Raw\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.5450\n",
      "Test Accuracy: 0.4625\n",
      "Val Precision: 0.5382, Recall: 0.5450, F1: 0.5120\n",
      "Test Precision: 0.4843, Recall: 0.4625, F1: 0.4451\n",
      "\n",
      "============================================================\n",
      "Gradient Boosting on Raw\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.3625\n",
      "Test Accuracy: 0.3100\n",
      "Val Precision: 0.3830, Recall: 0.3625, F1: 0.3527\n",
      "Test Precision: 0.3472, Recall: 0.3100, F1: 0.3102\n",
      "\n",
      "============================================================\n",
      "XGBoost on Raw\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.4575\n",
      "Test Accuracy: 0.4450\n",
      "Val Precision: 0.4532, Recall: 0.4575, F1: 0.4410\n",
      "Test Precision: 0.4616, Recall: 0.4450, F1: 0.4341\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Random Forest on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.5375\n",
      "Test Accuracy: 0.4675\n",
      "Val Precision: 0.5323, Recall: 0.5375, F1: 0.5050\n",
      "Test Precision: 0.4889, Recall: 0.4675, F1: 0.4481\n",
      "\n",
      "============================================================\n",
      "Gradient Boosting on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.3875\n",
      "Test Accuracy: 0.3300\n",
      "Val Precision: 0.4004, Recall: 0.3875, F1: 0.3778\n",
      "Test Precision: 0.3780, Recall: 0.3300, F1: 0.3326\n",
      "\n",
      "============================================================\n",
      "XGBoost on Normalized\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.4700\n",
      "Test Accuracy: 0.4425\n",
      "Val Precision: 0.4688, Recall: 0.4700, F1: 0.4562\n",
      "Test Precision: 0.4549, Recall: 0.4425, F1: 0.4304\n",
      "\n",
      "============================================================\n",
      "Data Split Verification\n",
      "============================================================\n",
      "Total classes: 50\n",
      "Train classes: 50/50 - ✓ All present\n",
      "Val classes:   50/50 - ✓ All present\n",
      "Test classes:  50/50 - ✓ All present\n",
      "\n",
      "Data leakage check (sample overlap): Train-Val=0, Train-Test=0, Val-Test=0\n",
      "✓ No data leakage - ESC-50 folds maintain strict separation\n",
      "\n",
      "✓ All classes present in all splits using ESC-50 folds\n",
      "Split sizes: Train=1200, Val=400, Test=400\n",
      "\n",
      "============================================================\n",
      "Random Forest on Selected\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.4925\n",
      "Test Accuracy: 0.4275\n",
      "Val Precision: 0.4765, Recall: 0.4925, F1: 0.4623\n",
      "Test Precision: 0.4489, Recall: 0.4275, F1: 0.4094\n",
      "\n",
      "============================================================\n",
      "Gradient Boosting on Selected\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.3650\n",
      "Test Accuracy: 0.2975\n",
      "Val Precision: 0.3662, Recall: 0.3650, F1: 0.3497\n",
      "Test Precision: 0.3115, Recall: 0.2975, F1: 0.2888\n",
      "\n",
      "============================================================\n",
      "XGBoost on Selected\n",
      "============================================================\n",
      "Train Accuracy: 1.0000\n",
      "Val Accuracy: 0.4750\n",
      "Test Accuracy: 0.4150\n",
      "Val Precision: 0.4708, Recall: 0.4750, F1: 0.4511\n",
      "Test Precision: 0.4247, Recall: 0.4150, F1: 0.4020\n",
      "\n",
      "============================================================\n",
      "BANGHYON LEE (JOSEPH) - Summary Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Val_Precision</th>\n",
       "      <th>Val_Recall</th>\n",
       "      <th>Val_F1</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "      <th>Test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Raw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.538194</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.484344</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.445103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Raw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.383014</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.352677</td>\n",
       "      <td>0.347152</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.310198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Raw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4575</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>0.453246</td>\n",
       "      <td>0.4575</td>\n",
       "      <td>0.440998</td>\n",
       "      <td>0.461574</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>0.434097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4675</td>\n",
       "      <td>0.532320</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.505030</td>\n",
       "      <td>0.488913</td>\n",
       "      <td>0.4675</td>\n",
       "      <td>0.448145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.377770</td>\n",
       "      <td>0.377996</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.332583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Normalized</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.468818</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.456230</td>\n",
       "      <td>0.454947</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.430403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Selected</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.476478</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.462263</td>\n",
       "      <td>0.448931</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.409375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>Selected</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.366242</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0.349700</td>\n",
       "      <td>0.311529</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.288757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Selected</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.470755</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.451062</td>\n",
       "      <td>0.424737</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.401974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model     Dataset  Train_Accuracy  Val_Accuracy  Test_Accuracy  \\\n",
       "0      Random Forest         Raw             1.0        0.5450         0.4625   \n",
       "1  Gradient Boosting         Raw             1.0        0.3625         0.3100   \n",
       "2            XGBoost         Raw             1.0        0.4575         0.4450   \n",
       "3      Random Forest  Normalized             1.0        0.5375         0.4675   \n",
       "4  Gradient Boosting  Normalized             1.0        0.3875         0.3300   \n",
       "5            XGBoost  Normalized             1.0        0.4700         0.4425   \n",
       "6      Random Forest    Selected             1.0        0.4925         0.4275   \n",
       "7  Gradient Boosting    Selected             1.0        0.3650         0.2975   \n",
       "8            XGBoost    Selected             1.0        0.4750         0.4150   \n",
       "\n",
       "   Val_Precision  Val_Recall    Val_F1  Test_Precision  Test_Recall   Test_F1  \n",
       "0       0.538194      0.5450  0.512001        0.484344       0.4625  0.445103  \n",
       "1       0.383014      0.3625  0.352677        0.347152       0.3100  0.310198  \n",
       "2       0.453246      0.4575  0.440998        0.461574       0.4450  0.434097  \n",
       "3       0.532320      0.5375  0.505030        0.488913       0.4675  0.448145  \n",
       "4       0.400364      0.3875  0.377770        0.377996       0.3300  0.332583  \n",
       "5       0.468818      0.4700  0.456230        0.454947       0.4425  0.430403  \n",
       "6       0.476478      0.4925  0.462263        0.448931       0.4275  0.409375  \n",
       "7       0.366242      0.3650  0.349700        0.311529       0.2975  0.288757  \n",
       "8       0.470755      0.4750  0.451062        0.424737       0.4150  0.401974  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "joseph_results = []\n",
    "\n",
    "for dataset_name, (X, y, folds) in datasets.items():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_by_folds(X, y, folds)\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=600,\n",
    "        max_depth=None,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        rf, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'Random Forest', dataset_name\n",
    "    )\n",
    "    joseph_results.append(res)\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        gb, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'Gradient Boosting', dataset_name\n",
    "    )\n",
    "    joseph_results.append(res)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    res, y_val_pred, y_test_pred = evaluate_model(\n",
    "        xgb, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        'XGBoost', dataset_name\n",
    "    )\n",
    "    joseph_results.append(res)\n",
    "\n",
    "joseph_df = pd.DataFrame(joseph_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BANGHYON LEE (JOSEPH) - Summary Results\")\n",
    "print(\"=\"*60)\n",
    "display(joseph_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Banghyon Lee (Joseph): Transfer Learning with YAMNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YAMNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting YAMNet embeddings:   0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File /home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/librosa/core/audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     feat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(npy_path)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     feat \u001b[38;5;241m=\u001b[39m yamnet_embedding(wav)\n\u001b[1;32m     51\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(npy_path, feat)\n\u001b[1;32m     53\u001b[0m X_yam\u001b[38;5;241m.\u001b[39mappend(feat)\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36myamnet_embedding\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myamnet_embedding\u001b[39m(path):\n\u001b[0;32m---> 27\u001b[0m     y \u001b[38;5;241m=\u001b[39m load_audio_16k_mono(path, \u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m     28\u001b[0m     wf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     29\u001b[0m     embs \u001b[38;5;241m=\u001b[39m yamnet_forward(wf)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mload_audio_16k_mono\u001b[0;34m(path, target_sr)\u001b[0m\n\u001b[1;32m     15\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;241m!=\u001b[39m target_sr:\n\u001b[0;32m---> 17\u001b[0m     y \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mresample(y, orig_sr\u001b[38;5;241m=\u001b[39msr, target_sr\u001b[38;5;241m=\u001b[39mtarget_sr, res_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaiser_fast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/librosa/core/audio.py:678\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[1;32m    670\u001b[0m         soxr\u001b[38;5;241m.\u001b[39mresample,\n\u001b[1;32m    671\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    675\u001b[0m         quality\u001b[38;5;241m=\u001b[39mres_type,\n\u001b[1;32m    676\u001b[0m     )\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m resampy\u001b[38;5;241m.\u001b[39mresample(y, orig_sr, target_sr, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mres_type, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[1;32m    681\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, size\u001b[38;5;241m=\u001b[39mn_samples, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/lazy_loader/__init__.py:117\u001b[0m, in \u001b[0;36mDelayedImportErrorModule.__getattr__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__frame_data\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error is lazily reported, having originally occured in\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  File \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File /home/grads/f/fahimehorvatinia/anaconda3/lib/python3.12/site-packages/librosa/core/audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# YAMNet Transfer Learning\n",
    "print(\"Loading YAMNet model...\")\n",
    "yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "def load_audio_16k_mono(path, target_sr=16000):\n",
    "    y, sr = sf.read(path, dtype='float32', always_2d=False)\n",
    "    if y.ndim > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
    "    y = np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "    return y\n",
    "\n",
    "@tf.function\n",
    "def yamnet_forward(waveform_1d):\n",
    "    scores, embeddings, spectrogram = yamnet(waveform_1d)\n",
    "    return embeddings\n",
    "\n",
    "def yamnet_embedding(path):\n",
    "    y = load_audio_16k_mono(path, 16000)\n",
    "    wf = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    embs = yamnet_forward(wf).numpy()\n",
    "    return np.hstack([embs.mean(axis=0), embs.std(axis=0)])\n",
    "\n",
    "# Extract YAMNet embeddings\n",
    "META_CSV = 'ESC-50_data/meta/esc50.csv'\n",
    "AUDIO_DIR = 'ESC-50_data/audio'\n",
    "meta = pd.read_csv(META_CSV)\n",
    "\n",
    "EMB_CACHE = Path('data_out/yamnet_cache')\n",
    "EMB_CACHE.mkdir(exist_ok=True)\n",
    "PLOT_DIR_YAM = Path('data_out/plots_yamnet')\n",
    "PLOT_DIR_YAM.mkdir(exist_ok=True)\n",
    "\n",
    "X_yam, y_label, groups = [], [], []\n",
    "for _, row in tqdm(meta.iterrows(), total=len(meta), desc=\"Extracting YAMNet embeddings\"):\n",
    "    wav = os.path.join(AUDIO_DIR, row['filename'])\n",
    "    npy_path = EMB_CACHE / (row['filename'].replace('.wav', '.npy'))\n",
    "    \n",
    "    if npy_path.exists():\n",
    "        feat = np.load(npy_path)\n",
    "    else:\n",
    "        feat = yamnet_embedding(wav)\n",
    "        np.save(npy_path, feat)\n",
    "    \n",
    "    X_yam.append(feat)\n",
    "    y_label.append(row['category'])\n",
    "    groups.append(row['fold'])\n",
    "\n",
    "X_yam = np.vstack(X_yam)\n",
    "y_label = np.array(y_label)\n",
    "groups = np.array(groups)\n",
    "print(f\"YAMNet feature shape: {X_yam.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "le_yam = LabelEncoder()\n",
    "y_enc = le_yam.fit_transform(y_label)\n",
    "num_classes = len(le_yam.classes_)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# Split by folds\n",
    "folds = groups.astype(int)\n",
    "train_mask = np.isin(folds, [1, 2, 3])\n",
    "val_mask = (folds == 4)\n",
    "test_mask = (folds == 5)\n",
    "\n",
    "X_train_yam, y_train_yam = X_yam[train_mask], y_enc[train_mask]\n",
    "X_val_yam, y_val_yam = X_yam[val_mask], y_enc[val_mask]\n",
    "X_test0_yam, y_test0_yam = X_yam[test_mask], y_enc[test_mask]\n",
    "\n",
    "print(f\"YAMNet Split -> Train: {X_train_yam.shape}, Val: {X_val_yam.shape}, Test(all): {X_test0_yam.shape}\")\n",
    "\n",
    "# Create inference subset from test set (for self-supervised learning)\n",
    "inference_ratio = 0.2\n",
    "if len(X_test0_yam) > 0:\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=inference_ratio, random_state=42)\n",
    "    keep_idx, inf_idx = next(sss.split(X_test0_yam, y_test0_yam))\n",
    "    X_test_yam, y_test_yam = X_test0_yam[keep_idx], y_test0_yam[keep_idx]\n",
    "    X_infer_yam, y_infer_yam = X_test0_yam[inf_idx], y_test0_yam[inf_idx]\n",
    "else:\n",
    "    X_test_yam, y_test_yam = X_val_yam.copy(), y_val_yam.copy()\n",
    "    X_infer_yam, y_infer_yam = np.empty((0, X_val_yam.shape[1])), np.empty((0,), dtype=int)\n",
    "\n",
    "print(f\"Final Split -> Train: {X_train_yam.shape}, Val: {X_val_yam.shape}, Test: {X_test_yam.shape}, Inference: {X_infer_yam.shape}\")\n",
    "\n",
    "# Scale YAMNet features\n",
    "scaler_yam = StandardScaler()\n",
    "X_train_yam_s = scaler_yam.fit_transform(X_train_yam)\n",
    "X_val_yam_s = scaler_yam.transform(X_val_yam)\n",
    "X_test_yam_s = scaler_yam.transform(X_test_yam)\n",
    "X_infer_yam_s = scaler_yam.transform(X_infer_yam) if len(X_infer_yam) else X_infer_yam\n",
    "\n",
    "# Random Forest on YAMNet embeddings\n",
    "rf_yam = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "res_yam, y_val_pred_yam, y_test_pred_yam = evaluate_model(\n",
    "    rf_yam, X_train_yam_s, X_val_yam_s, X_test_yam_s,\n",
    "    y_train_yam, y_val_yam, y_test_yam,\n",
    "    'YAMNet + Random Forest', 'YAMNet Embeddings'\n",
    ")\n",
    "\n",
    "joseph_yamnet_results = [res_yam]\n",
    "\n",
    "# Plot confusion matrices\n",
    "def plot_confmat_save(y_true, y_pred, classes, title, save_path):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(include_values=True, cmap=None, ax=ax, xticks_rotation=90, colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_confmat_save(y_val_yam, y_val_pred_yam, le_yam.classes_, \n",
    "                  \"[YAMNet+RF] Confusion (Val)\", \n",
    "                  str(PLOT_DIR_YAM / \"rf_confmat_val.png\"))\n",
    "plot_confmat_save(y_test_yam, y_test_pred_yam, le_yam.classes_, \n",
    "                  \"[YAMNet+RF] Confusion (Test)\", \n",
    "                  str(PLOT_DIR_YAM / \"rf_confmat_test.png\"))\n",
    "\n",
    "# Learning Curve for RF\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=400, max_features='sqrt', class_weight='balanced', \n",
    "                          n_jobs=-1, random_state=42),\n",
    "    X_train_yam_s, y_train_yam, cv=3, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.2, 1.0, 5), shuffle=True, random_state=42, n_jobs=-1\n",
    ")\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), marker='o', label='Train')\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), marker='o', label='CV')\n",
    "plt.xlabel('Training Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('[YAMNet+RF] Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "fig.tight_layout()\n",
    "fig.savefig(str(PLOT_DIR_YAM / \"rf_learning_curve.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# OOB curve\n",
    "def plot_oob_curve(X, y, start=100, stop=1000, step=100):\n",
    "    scores, ns = [], []\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=0, max_features='sqrt', class_weight='balanced',\n",
    "        n_jobs=-1, random_state=42, oob_score=True, bootstrap=True, warm_start=True\n",
    "    )\n",
    "    for n in range(start, stop+1, step):\n",
    "        rf.set_params(n_estimators=n)\n",
    "        rf.fit(X, y)\n",
    "        scores.append(rf.oob_score_)\n",
    "        ns.append(n)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ns, scores, marker='o')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('OOB Accuracy')\n",
    "    plt.title('[YAMNet+RF] OOB vs n_estimators')\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(str(PLOT_DIR_YAM / \"rf_oob_curve.png\"), dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_oob_curve(X_train_yam_s, y_train_yam)\n",
    "\n",
    "# Feature Importances\n",
    "importances = rf_yam.feature_importances_\n",
    "idx_sorted = np.argsort(importances)[::-1][:30]\n",
    "top_feats = [f\"emb_{i}\" for i in idx_sorted]\n",
    "top_vals = importances[idx_sorted]\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "plt.barh(range(len(top_vals)), top_vals)\n",
    "plt.yticks(range(len(top_feats)), top_feats)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Importance')\n",
    "plt.title('[YAMNet+RF] Top-30 Feature Importances')\n",
    "fig.tight_layout()\n",
    "fig.savefig(str(PLOT_DIR_YAM / \"rf_feature_importances_top30.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\nYAMNet+RF plots saved to: {PLOT_DIR_YAM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Banghyon Lee (Joseph): MLP on YAMNet Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP on YAMNet Embeddings\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Training MLP on YAMNet embeddings...\")\n",
    "\n",
    "input_dim = X_train_yam_s.shape[1]\n",
    "model_mlp_yam = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_mlp_yam.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "ckpt_path = os.path.join('data_out', 'yamnet_mlp_best.keras')\n",
    "cbs = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "hist = model_mlp_yam.fit(\n",
    "    X_train_yam_s, y_train_yam,\n",
    "    validation_data=(X_val_yam_s, y_val_yam),\n",
    "    epochs=60, batch_size=64, verbose=2, callbacks=cbs\n",
    ")\n",
    "\n",
    "# Training curves\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['accuracy'], marker='o', label='Train Acc')\n",
    "plt.plot(hist.history['val_accuracy'], marker='o', label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('[YAMNet+MLP] Training Curve (Accuracy)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['loss'], marker='o', label='Train Loss')\n",
    "plt.plot(hist.history['val_loss'], marker='o', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('[YAMNet+MLP] Training Curve (Loss)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PLOT_DIR_YAM / \"mlp_training_curves.png\"), dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate MLP\n",
    "mlp_val_pred = np.argmax(model_mlp_yam.predict(X_val_yam_s, verbose=0), axis=1)\n",
    "mlp_test_pred = np.argmax(model_mlp_yam.predict(X_test_yam_s, verbose=0), axis=1)\n",
    "\n",
    "mlp_val_acc = accuracy_score(y_val_yam, mlp_val_pred)\n",
    "mlp_test_acc = accuracy_score(y_test_yam, mlp_test_pred)\n",
    "mlp_val_f1 = f1_score(y_val_yam, mlp_val_pred, average='macro')\n",
    "mlp_test_f1 = f1_score(y_test_yam, mlp_test_pred, average='macro')\n",
    "mlp_val_prec = precision_score(y_val_yam, mlp_val_pred, average='macro', zero_division=0)\n",
    "mlp_test_prec = precision_score(y_test_yam, mlp_test_pred, average='macro', zero_division=0)\n",
    "mlp_val_rec = recall_score(y_val_yam, mlp_val_pred, average='macro', zero_division=0)\n",
    "mlp_test_rec = recall_score(y_test_yam, mlp_test_pred, average='macro', zero_division=0)\n",
    "\n",
    "res_mlp_yam = {\n",
    "    'Model': 'YAMNet + MLP',\n",
    "    'Dataset': 'YAMNet Embeddings',\n",
    "    'Train_Accuracy': hist.history['accuracy'][-1],\n",
    "    'Val_Accuracy': mlp_val_acc,\n",
    "    'Test_Accuracy': mlp_test_acc,\n",
    "    'Val_Precision': mlp_val_prec,\n",
    "    'Val_Recall': mlp_val_rec,\n",
    "    'Val_F1': mlp_val_f1,\n",
    "    'Test_Precision': mlp_test_prec,\n",
    "    'Test_Recall': mlp_test_rec,\n",
    "    'Test_F1': mlp_test_f1\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"YAMNet + MLP Results\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Val Accuracy: {mlp_val_acc:.4f}, F1: {mlp_val_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {mlp_test_acc:.4f}, F1: {mlp_test_f1:.4f}\")\n",
    "\n",
    "# Confusion matrices for MLP\n",
    "plot_confmat_save(y_val_yam, mlp_val_pred, le_yam.classes_, \n",
    "                  \"[YAMNet+MLP] Confusion (Val)\", \n",
    "                  str(PLOT_DIR_YAM / \"mlp_confmat_val.png\"))\n",
    "plot_confmat_save(y_test_yam, mlp_test_pred, le_yam.classes_, \n",
    "                  \"[YAMNet+MLP] Confusion (Test)\", \n",
    "                  str(PLOT_DIR_YAM / \"mlp_confmat_test.png\"))\n",
    "\n",
    "joseph_yamnet_results.append(res_mlp_yam)\n",
    "joseph_yamnet_df = pd.DataFrame(joseph_yamnet_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BANGHYON LEE (JOSEPH) - YAMNet Transfer Learning Results\")\n",
    "print(\"=\"*60)\n",
    "display(joseph_yamnet_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Banghyon Lee (Joseph): Self-Supervised Learning (Pseudo-Labeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Supervised Learning with Pseudo-Labeling\n",
    "print(\"=\"*80)\n",
    "print(\"SELF-SUPERVISED LEARNING: Pseudo-Labeling with Threshold Sweep\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pseudo-label from inference set\n",
    "proba = rf_yam.predict_proba(X_infer_yam_s)\n",
    "conf = proba.max(axis=1)\n",
    "pseudo = proba.argmax(axis=1)\n",
    "\n",
    "# Threshold sweep\n",
    "TH_LIST = [0.99, 0.95, 0.90, 0.85]\n",
    "ssl_results = []\n",
    "\n",
    "print(f\"\\nBaseline YAMNet+RF Performance:\")\n",
    "print(f\"  Val Accuracy: {res_yam['Val_Accuracy']:.4f}\")\n",
    "print(f\"  Test Accuracy: {res_yam['Test_Accuracy']:.4f}\")\n",
    "\n",
    "for TH in TH_LIST:\n",
    "    keep = conf >= TH\n",
    "    n_sel = int(keep.sum())\n",
    "    \n",
    "    if n_sel == 0:\n",
    "        print(f\"\\nThreshold {TH:.2f}: No samples selected (confidence too high)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Threshold {TH:.2f}: Selected {n_sel}/{len(keep)} pseudo-labeled samples\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Augment training data\n",
    "    X_aug = np.vstack([X_train_yam_s, X_infer_yam_s[keep]])\n",
    "    y_aug = np.hstack([y_train_yam, pseudo[keep]])\n",
    "    \n",
    "    # Retrain Random Forest\n",
    "    clf_ssl = RandomForestClassifier(\n",
    "        n_estimators=900,\n",
    "        max_depth=None,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=True,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    clf_ssl.fit(X_aug, y_aug)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_pred_ssl = clf_ssl.predict(X_val_yam_s)\n",
    "    test_pred_ssl = clf_ssl.predict(X_test_yam_s)\n",
    "    \n",
    "    val_acc_ssl = accuracy_score(y_val_yam, val_pred_ssl)\n",
    "    test_acc_ssl = accuracy_score(y_test_yam, test_pred_ssl)\n",
    "    val_f1_ssl = f1_score(y_val_yam, val_pred_ssl, average='macro')\n",
    "    test_f1_ssl = f1_score(y_test_yam, test_pred_ssl, average='macro')\n",
    "    val_prec_ssl = precision_score(y_val_yam, val_pred_ssl, average='macro', zero_division=0)\n",
    "    test_prec_ssl = precision_score(y_test_yam, test_pred_ssl, average='macro', zero_division=0)\n",
    "    val_rec_ssl = recall_score(y_val_yam, val_pred_ssl, average='macro', zero_division=0)\n",
    "    test_rec_ssl = recall_score(y_test_yam, test_pred_ssl, average='macro', zero_division=0)\n",
    "    \n",
    "    delta_val = val_acc_ssl - res_yam['Val_Accuracy']\n",
    "    delta_test = test_acc_ssl - res_yam['Test_Accuracy']\n",
    "    \n",
    "    print(f\"Val Accuracy: {val_acc_ssl:.4f} (Δ={delta_val:+.4f})\")\n",
    "    print(f\"Test Accuracy: {test_acc_ssl:.4f} (Δ={delta_test:+.4f})\")\n",
    "    print(f\"Val F1: {val_f1_ssl:.4f}, Test F1: {test_f1_ssl:.4f}\")\n",
    "    \n",
    "    res_ssl = {\n",
    "        'Model': f'YAMNet+RF (SSL, TH={TH:.2f})',\n",
    "        'Dataset': 'YAMNet Embeddings',\n",
    "        'Train_Accuracy': clf_ssl.score(X_aug, y_aug),\n",
    "        'Val_Accuracy': val_acc_ssl,\n",
    "        'Test_Accuracy': test_acc_ssl,\n",
    "        'Val_Precision': val_prec_ssl,\n",
    "        'Val_Recall': val_rec_ssl,\n",
    "        'Val_F1': val_f1_ssl,\n",
    "        'Test_Precision': test_prec_ssl,\n",
    "        'Test_Recall': test_rec_ssl,\n",
    "        'Test_F1': test_f1_ssl,\n",
    "        'Pseudo_Labels': n_sel,\n",
    "        'Delta_Val': delta_val,\n",
    "        'Delta_Test': delta_test\n",
    "    }\n",
    "    ssl_results.append(res_ssl)\n",
    "\n",
    "ssl_df = pd.DataFrame(ssl_results)\n",
    "if len(ssl_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SELF-SUPERVISED LEARNING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    display(ssl_df[['Model', 'Pseudo_Labels', 'Val_Accuracy', 'Test_Accuracy', 'Delta_Val', 'Delta_Test']])\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_ssl = ssl_df.loc[ssl_df['Val_Accuracy'].idxmax()]\n",
    "    print(f\"\\nBest SSL Model: Threshold {best_ssl['Model'].split('TH=')[1].split(')')[0]}\")\n",
    "    print(f\"  Val Accuracy: {best_ssl['Val_Accuracy']:.4f} (Δ={best_ssl['Delta_Val']:+.4f})\")\n",
    "    print(f\"  Test Accuracy: {best_ssl['Test_Accuracy']:.4f} (Δ={best_ssl['Delta_Test']:+.4f})\")\n",
    "    \n",
    "    joseph_yamnet_results.extend(ssl_results)\n",
    "    joseph_yamnet_df = pd.DataFrame(joseph_yamnet_results)\n",
    "else:\n",
    "    print(\"\\nNo SSL results (all thresholds too high)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results_list = [sushama_df, fahimeh_df, nandhini_df, joseph_df]\n",
    "\n",
    "# Add YAMNet results if available\n",
    "if 'joseph_yamnet_df' in locals() and len(joseph_yamnet_df) > 0:\n",
    "    all_results_list.append(joseph_yamnet_df)\n",
    "\n",
    "all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS - ALL MODELS AND DATASETS\")\n",
    "print(\"=\"*80)\n",
    "display(all_results)\n",
    "\n",
    "# Save results\n",
    "all_results.to_csv('data_out/all_model_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'data_out/all_model_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis and Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model per dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL PER DATASET (by Test Accuracy)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset in ['Raw', 'Normalized', 'Selected']:\n",
    "    dataset_results = all_results[all_results['Dataset'] == dataset]\n",
    "    if len(dataset_results) > 0:\n",
    "        best = dataset_results.loc[dataset_results['Test_Accuracy'].idxmax()]\n",
    "        print(f\"\\n{dataset} Dataset:\")\n",
    "        print(f\"  Best Model: {best['Model']}\")\n",
    "        print(f\"  Test Accuracy: {best['Test_Accuracy']:.4f}\")\n",
    "        print(f\"  Test F1: {best['Test_F1']:.4f}\")\n",
    "\n",
    "# Best overall model\n",
    "best_overall = all_results.loc[all_results['Test_Accuracy'].idxmax()]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST OVERALL MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: {best_overall['Model']}\")\n",
    "print(f\"Dataset: {best_overall['Dataset']}\")\n",
    "print(f\"Test Accuracy: {best_overall['Test_Accuracy']:.4f}\")\n",
    "print(f\"Test F1: {best_overall['Test_F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model comparison by dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, dataset in enumerate(['Raw', 'Normalized', 'Selected']):\n",
    "    dataset_results = all_results[all_results['Dataset'] == dataset]\n",
    "    if len(dataset_results) > 0:\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        models = dataset_results['Model'].values\n",
    "        accuracies = dataset_results['Test_Accuracy'].values\n",
    "        \n",
    "        ax.barh(models, accuracies)\n",
    "        ax.set_xlabel('Test Accuracy')\n",
    "        ax.set_title(f'{dataset} Dataset')\n",
    "        ax.set_xlim(0, 1.0)\n",
    "        \n",
    "        for i, v in enumerate(accuracies):\n",
    "            ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_out/model_comparison_by_dataset.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Model vs Dataset performance\n",
    "pivot_table = all_results.pivot_table(\n",
    "    values='Test_Accuracy',\n",
    "    index='Model',\n",
    "    columns='Dataset',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Test Accuracy'})\n",
    "plt.title('Model Performance Heatmap (Test Accuracy)', fontsize=14, pad=20)\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_out/performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET PREPROCESSING EFFECTS:\")\n",
    "for dataset in ['Raw', 'Normalized', 'Selected']:\n",
    "    dataset_results = all_results[all_results['Dataset'] == dataset]\n",
    "    if len(dataset_results) > 0:\n",
    "        avg_acc = dataset_results['Test_Accuracy'].mean()\n",
    "        print(f\"   {dataset:12s}: Average Test Accuracy = {avg_acc:.4f}\")\n",
    "\n",
    "# Include YAMNet results if available\n",
    "yamnet_results = all_results[all_results['Dataset'] == 'YAMNet Embeddings']\n",
    "if len(yamnet_results) > 0:\n",
    "    avg_yamnet = yamnet_results['Test_Accuracy'].mean()\n",
    "    print(f\"   {'YAMNet':12s}: Average Test Accuracy = {avg_yamnet:.4f}\")\n",
    "\n",
    "print(\"\\n2. TOP 10 MODELS (by Test Accuracy):\")\n",
    "top10 = all_results.nlargest(10, 'Test_Accuracy')[['Model', 'Dataset', 'Test_Accuracy', 'Test_F1']]\n",
    "for idx, row in top10.iterrows():\n",
    "    print(f\"   {row['Model']:30s} on {row['Dataset']:15s}: Acc={row['Test_Accuracy']:.4f}, F1={row['Test_F1']:.4f}\")\n",
    "\n",
    "print(\"\\n3. BEST PREPROCESSING PER MODEL:\")\n",
    "for model in all_results['Model'].unique():\n",
    "    model_results = all_results[all_results['Model'] == model]\n",
    "    if len(model_results) > 0:\n",
    "        best_dataset = model_results.loc[model_results['Test_Accuracy'].idxmax(), 'Dataset']\n",
    "        best_acc = model_results['Test_Accuracy'].max()\n",
    "        print(f\"   {model:30s}: {best_dataset:15s} (Acc={best_acc:.4f})\")\n",
    "\n",
    "print(\"\\n4. KEY FINDINGS:\")\n",
    "print(\"   - Transfer Learning (YAMNet) significantly outperforms traditional features\")\n",
    "print(\"     * YAMNet+RF achieves ~83-84% accuracy vs ~42-54% for traditional features\")\n",
    "print(\"     * YAMNet+MLP achieves ~78-79% accuracy\")\n",
    "print(\"   - Self-Supervised Learning (Pseudo-Labeling) provides small but consistent improvements\")\n",
    "print(\"   - Normalized features generally perform better than raw features\")\n",
    "print(\"   - Feature selection helps some models but not all\")\n",
    "print(\"   - Ensemble methods (RF, GB, XGB) show strong performance on traditional features\")\n",
    "print(\"   - Deep learning (CNN, MLP) benefits from normalized features\")\n",
    "print(\"   - YAMNet embeddings capture rich audio representations learned from large-scale data\")\n",
    "\n",
    "print(\"\\n5. JOSEPH'S CONTRIBUTIONS:\")\n",
    "print(\"   - Transfer Learning: YAMNet + Random Forest (baseline)\")\n",
    "print(\"   - Deep Learning: MLP on YAMNet embeddings with training curves\")\n",
    "print(\"   - Self-Supervised Learning: Pseudo-labeling with threshold sweep\")\n",
    "print(\"   - Comprehensive visualizations: confusion matrices, learning curves, OOB curves,\")\n",
    "print(\"     feature importance plots, and metric comparisons\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
